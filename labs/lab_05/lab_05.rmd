---
title: "lab_05"
author: "derek willis"
date: "2023-03-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

-   Tabula

## Load libraries and establish settings

```{r}
# Turn off scientific notation
options(scipen=999)

# Load the tidyverse.
library(tidyverse)
library(lubridate)
library(janitor)
```

## Get Our PDF

We'll be working with the [911 overdose calls from Baltimore County](https://drive.google.com/file/d/1qkYuojGF_6WKFr5aNQxmewDzcKyOiJFr/view?usp=share_link). You'll want to download it to a place you'll remember (like your Downloads folder, or the labs folder in your repository). The goal is to extract the tables within it, export that to a CSV file, load it into RStudio and ask some questions.

## Extract Data from PDF Using Tabula

Start Tabula, then go to <http://127.0.0.1:8080/> in your browser. Click the "Browse" button and find the PDF file and click "open", and then click the "Import button" in Tabula. This will take a few seconds or longer.

This PDF has a single table spread over multiple pages to extract. We're going to make a single dataframe from this table, exporting it to a CSV file that you will load into R. In Tabula, highlight the table and click the "Preview & Export Extracted Data" button. You may want to play with including or excluding the column headers - YOU SHOULD HAVE FIVE COLUMNS OF DATA.

Save the CSV (it should be called `tabula-Baltimore County; Carey, Samantha log OD.csv` by default) to your lab_05/data folder.

From there, you will need to read in the data, and add or fix headers if necessary. You can choose to include the headers from the PDF in your exported CSV files OR to exclude them and add them when importing. `read_csv` allows us to do this ([and more](https://readr.tidyverse.org/reference/read_delim.html)).

## Load and clean up the data in R

You will need to read in and clean up the data so that it can be used for analysis. By "clean" I mean the column headers should not contain spaces and they should have meaningful names, not "x1" or something similar. How you do that is up to you, but you can use select() with or without the minus sign to include or exclude certain columns. You also can use the `rename` function to, well, rename columns. Importantly, you'll need to ensure that any columns containing a date actually have a date datatype. Our friend `lubridate` can help with this.

```{r}

emergency_calls <- read_csv("tabula-Baltimore County; Carey, Samantha log OD.csv", col_name = FALSE) |>
  clean_names()|>
  rename( date=x1, time=x2, phone_number=x3, case_number=x4, address=x5)|>
  mutate(date = mdy(date))
```

## Answer questions

Q1. Write code to generate the number of calls that occurred on each date. Which date in 2022 had the most overdose calls, and how many? Look at the total number of rows in your result and explore the range of dates - based on your result, do you believe there are any days with no overdose calls at all? Explain why or why not.

A1. It appears that 07/14/2022 is tied with 10/04/2022 for the most calls, both sitting at 23 total. Regarding days with no calls, arranging this code by descending order of date instead of calls reveals that the first call was recorded 02/06/2022, and the last on 02/06/2023. That's an even 365 days, and when counting the first date (02/06/2022), it should come out to 366. Wouldn't you know it, the code below comes out to 366 rows. With this in mind, I'm pretty positive that, depressing as it sounds, there wasn't a single day in the 2022-2023 records that didn't see an overdose report.

```{r}
emergency_calls |>
  group_by(date)|>
  summarise(
    calls_total=n()
  )|>
arrange(desc(calls_total))
```

Q2. You want to understand if there's a pattern in the day of the week that overdose calls are made. Add a column to your dataframe that displays what day of the week each date represents. You should search for how to do that using lubridate. Then write code to calculate the number of calls for each day of the week, and add a column to that result that calculates the percentage of all calls that occurred on each day of the week (so you want a dataframe with the day of the week, total number of calls and the percentage of calls on that day out of the total number of all calls). Describe your findings to me.

A2. According to my code, Saturday and Sunday have the highest percentage of overdose calls at 15.51% and 15.1%, respectively. This percentage drops off gradually until we reach Thursday, lowest in the data, sitting at 12.79%. Makes sense to me – people would most likely be using drugs the most on their days off, which says something about how disturbingly mundane addiction can be.

```{r}
emergency_calls |>
  mutate(weekdays = wday(ymd(date), label = TRUE))|>
  group_by(weekdays)|>
  summarise(
    calls=n()
  )|>
arrange(desc(calls))|>
  mutate(pct_calls=calls/sum(calls)*100)
  #mutate(pct_calls=calls/sum_calls*100)
  
```

Q3. Now let's look at locations. Which ones have the most calls? How would you describe them (feel free to search for more information on them)? Is there anything about the structure of the original data that might make you less confident in the counts by location or date?

A3. It looks to me like 4540 Silver Spring Road is has the most call counts overall, which looks to be an address in the White Marsh suburb of Baltimore. It makes sense – according to Google, Baltimore represents 44% of the overdose deaths in MD, even though it's only 9% of the population. Strangely enough, the 2nd and 3rd highest rates are both from addresses with a "PC" label. PC 02; 6424 Windsor Mill, as well as PC 06; 115 Susquehanna Av W, both appear to be police departments. Now, I can only surmise as to why calls are coming *from* the police department, but my guess would be that some overdose calls are being reported there rather than their source location. As such, although the data set seems quite strong, I have to wonder how often this problem comes up across the 4112 observations – that is to say, how often is a call coming from a shelter, police department, etc, rather than the actual overdose source?

```{r}
emergency_calls |>
  group_by(address)|>
  summarise(
    count = n()
    )|>
arrange(desc(count))

```

Q4. What's the best story idea or question you've seen as a result of the work you've done in this lab?

A4. I'd be quite compelled to look a little deeper into percentages and rates within the emergency_calls dataset. If you wanted to do more research (a lot more, it might take some significant work), you could mutate a new column to represent the county, city, town, and/or unincorporated township for each call. From there, you could quite easily find which sections of Maryland represent the most rampant drug overdose calls, at least within the source. From here, several avenues of inquiry open. One might ask "which areas are being impacted more than others? Which sub-sections of Baltimore/Washington County/Moco/PG County are being disproportionately represented in the data, and why might that be?" Of course, you could also do something a little more longitudinal. Here are the areas that were impacted up to 2022, now let's go find a new data set and compare with the old. Are things much the same, or have any percentages shifted?
